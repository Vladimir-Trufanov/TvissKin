## [Проект нейронной сети на Python](NeuralNet/NeuralNet.ipynb)

### Весовые коэффициенты - сердце сети

Наиболее важная часть сети — весовые коэффициенты связей (веса). Они используются для расчета распространения сигналов в прямом направлении, а также обратного распространения ошибок, и именно весовые коэффициенты уточняются 
в попытке улучшить характеристики сети.

Для компактной записи весов будем использовать следующие матрицы:

```
• матрицу весов для связей между входным и скрытым слоями:
     W входной_скрытый,  размерностью hidden_nodes * input_nodes;
• другую матрицу для связей между скрытым и выходным слоями:
    W скрытый_выходной, размерностью output_nodes * hidden_nodes.
```

Начальные значения весовых коэффициентов должны быть небольшими и выбираться случайным образом. Следующая функция из пакета numpy генерирует массив случайных чисел в диапазоне от 0 до 1, где размерность массива равна rows * columns:

```
numpy.random.rand(rows,columns)
```
Так как собираемся использовать расширение пакета numpy, мы должны импортировать эту библиотеку в самом начале кода:

```
import numpy
```
Данный подход можно улучшить, поскольку весовые коэффициенты могут иметь не только положительные, но и отрицательные значения, и изменяться в пределах от -1,0 до +1,0 не достигая граничных пределов. Для простоты мы вычтем 0,5 из этих граничных значений, перейдя к диапазону значений от -0,5 до +0,5. Можно сказать - изящное решение:

```
m = numpy.random.rand(3,3) - 0.5

```
Создаем две матрицы начальных весов, используя значения переменных self.inodes, self.hnodes и self.onodes для задания соответствующих размеров каждой из матриц. Эти весовые коэффициенты составляют неотъемлемую часть нейронной сети и служат ее характеристиками на протяжении всей ее жизни.

```
self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5) 
self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5)
```
Это было простое, но популярное решение инициализации весовых коэффициентов.

Есть усовершенствованный подход к созданию случайных начальных значений весов. Для этого весовые коэффициенты выбираются из нормального распределения с центром в нуле и со стандартным отклонением, величина которого обратно пропорциональна корню квадратному из количества входящих связей на узел (такой контекст интуитивно кажется более подходящий для нейронной сети, но не факт).

Функция ***numpy.random.normal()*** поможет нам с извлечением выборки из нормального распределения. Ее параметрами являются центр распределения, стандартное отклонение и размер массива numpy, так как нам нужна матрица 
случайных чисел, а не одиночное число:

```
self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5),(self.hnodes, self.inodes))
self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5),(self.onodes, self.hnodes))
```
Как видно, центр нормального распределения установлен здесь в 0,0. Стандартное отклонение вычисляется по количеству узлов в следующем слое с помощью функции pow(self.hnodes, - 0.5), которая просто возводит количество узлов в степень -0 ,5. Последний параметр определяет конфигурацию массива numpy.

### Опрос сети

Опросом сети занимается функция query(), которая принимает в качестве аргумента входные данные нейронной сети и возвращает ее выходные данные. По аналогии с искусственным нейроном здесь передаются сигналы от узлов входного слоя через скрытый слой к узлам выходного слоя для получения выходных сигналов. 

В нашем случае, например, можно получить входящие сигналы для узлов скрытого слоя путем умножения матрицы весовых коэффициентов связей между входным и скрытым слоями на матрицу входных сигналов:

```
X скрытый = W скрытый_входной * I скрытый
```
При этом,  по мере распространения, сигналы сглаживаются через весовые коэффициенты связей между соответствующими узлами, а так же просчитывается сигмоида для уменьшения уровня выходных сигналов узлов.

В Python для передачи сигнала используется ***функция скалярного произведения*** матриц из библиотеки numpy к матрицам весов и входных сигналов:

```
hidden_inputs = numpy.dot(self.wih, inputs)
```

Библиотека ***scipy*** в Python содержит набор специальных функций, в том числе сигмоиду, которая называется expit (). Библиотека scipy импортируется точно так же, как и библиотека numpy:

```
import scipy.special
```
Определяем сигмоиду, как функцию активации нашей нейронной сети, следующим образом:

```
self.activation_function = lambda x: scipy.special.expit(x)
```
Здесь мы создали функцию, но вместо формы ***def имя()*** использовали волшебное слово ***lambda***,  как более короткий способ записи, называемый лямбда-выражением. Функция принимает аргумент х и возвращает scipy.special.expit(), а это есть не что иное, как сигмоида. Нашей функции мы присвоили имя self.activation _function(), так и будем ее вызывать.

### Тренировка сети

Код функции почти совпадает с кодом функции query(), поскольку процесс передачи сигнала от входного слоя к выходному остается одним и тем же. Единственным отличием является введение дополнительного параметра ***targets_list***, передаваемого при вызове функции, поскольку невозможно тренировать сеть без предоставления ей тренировочных примеров, которые включают желаемые или целевые значения. 

Теперь будем решать основную задачу тренировки сети — уточнять веса на основе расхождения между расчетными и целевыми значениями.

Прежде всего, вычисляем ошибку, являющуюся разностью между желаемым целевым выходным значением, предоставленным тренировочным примером, и фактическим выходным значением. Она представляет собой разность между матрицами ***(targets - final_outputs)***, рассчитываемую поэлементно. 
```
# ошибка = целевое значение - фактическое значение 
output_errors = targets - final_outputs
```
Далее рассчитываем обратное распространение ошибок для узлов скрытого слоя, распределяя их между узлами пропорционально весовым коэффициентам связей (через скалярное произведение матриц), а затем рекомбинируем их на каждом узле скрытого слоя. 

![Матричная формула расчета обратного распространения ошибок](matrichnaya-formula-rascheta-obratnogo-rasprostraneniya-oshibok.jpg)

```
# ошибки скрытого слоя - это ошибки output_errors,
# распределенные пропорционально весовым коэффициентам связей
# и рекомбинированные на скрытых узлах 
hidden_errors = numpy.dot(self.who.T, output_errors)
```
Теперь обновляем весовые коэффициенты в каждом слое. Для весов связей между скрытым и выходным слоями мы используем переменную output_errors. Для весов связей между входным и скрытым слоями мы используем только что рассчитанную переменную hidden_errors.
 
![Обновление веса связи между узлом j и к следующего слоя в матричной форме](obnovlenie-vesa-svyazi-mezhdu-uzlom-j-i-k-sleduyushchego-sloya-v-matrichnoj-forme.jpg)

Величина аlpha — это коэффициент обучения, а сигмоида — это функция активации, символ "*" означает обычное поэлементное умножение, а символ "." скалярное произведение матриц. Последний член выражения — это транспонированная "т" матрица исходящих сигналов предыдущего слоя. В данном случае транспонирование означает преобразование столбца выходных сигналов в строку.
```
# обновить весовые коэффициенты связей между скрытым и выходным слоями 
self.who += self.lr * numpy.dot((output_errors * final_outputs  * (1.0-final_outputs)),  numpy.transpose(hidden_outputs))

# обновить весовые коэффициенты связей между входным и скрытым слоями 
self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0-hidden_outputs)), numpy.transpose(inputs))
```

[Завершённый класс нейронной сети: https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/blob/master/part2_neural_network.ipynb](https://github.com/makeyourownneuralnetwork/makeyourownneuralnetwork/blob/master/part2_neural_network.ipynb)

### Наборы рукописных цифр MNIST (в формате CSV)

Существует коллекция изображений рукописных цифр, используемых исследователями искусственного интеллекта в качестве популярного набора для тестирования идей и алгоритмов “MNIST”, предоставленная исследователем нейронных сетей Яном Лекуном для бесплатного всеобщего доступа по адресу 
[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).

Формат базы данных MNIST не простой, но созданы соответствующие файлы в формате CSV:

[Тренировочный набор содержит 60 000 промаркированных экземпляров](http://www.pjreddie.com/media/files/mnist_train.csv), используемых для тренировки нейронной сети. Слово “промаркированные” означает, что для каждого экземпляра указан соответствующий правильный ответ.

[Меньший тестовый набор, включающий 10 000 экземпляров](http://www.pjreddie.com/media/files/mnist_test.csv), используется для проверки правильности работы идей или алгоритмов. Он также содержит корректные маркеры, позволяющие увидеть, способна ли наша нейронная сеть дать правильный ответ.

В каталоге ***mnist_dataset*** находятся файлы MNIST, также подготовленные в формате CSV:

• [10 записей из тестового набора данных MNIST](mnist_dataset/mnist_test_10.csv)

• [100 записей из тренировочного набора данных MNIST](mnist_dataset/mnist_train_l00.csv)

Содержимое этих записей, т.е. строк текста, легко понять.

• Первое значение — это маркер, т.е. фактическая цифра, например “7” или “9”, которую должен представлять данный рукописный экземпляр. Это ответ, правильному получению которого должна обучиться нейронная сеть.

• Последующие значения, разделенные запятыми, — это значения пикселей рукописной цифры. Пиксельный массив имеет размерность 28x28, поэтому за каждым маркером следуют 784 пикселя.

Прежде чем с этими данными можно будет что-либо сделать, например построить график или обучить с их помощью нейронную сеть, необходимо обеспечить доступ к ним из кода на языке Python.

Открытие файлов и получение их содержимого в Python не составляет большого труда. 
```
# Читаем файл с помощью функции readlines(), это позволит в дальнейшем
# построчно обращаться к считанным данным:
#    data_list[0] — это первая запись, 
#    data_list[9] — десятая и т.п
data_file = open("mnist_dataset/mnist_train_100.csv",'r') 
data_list = data_file.readlines() 
data_file.close()
```



### Tensorflow обучение: введение в глубокое обучение - Андрей Шмиг

```
Практический курс глубокого обучения для разработчиков программного обеспечения.
Стоимость курса: бесплатный
Длительность: примерно 2 месяца
Уровень: средний
```
О курсе

Научимся разрабатывать приложения с глубоким обучением с использованием  TensorFlow. Этот курс был разработан командой TensorFlow и Udacity в качестве практического подхода к глубокому обучению для разработчиков программного обеспечения. Вы погрузитесь и получите опыт разработки современного классификатора изображений и других моделей глубокого обучения. Так же у вас будет возможность воспользоваться моделями построенными при помощи TensorFlow в реальном мире на мобильных устройствах, в облаке, и браузере. В завершении, вы воспользуетесь продвинутыми техниками и алгоритмами для работы с большими объемами данных. К концу этого курса у вас будут все необходимые навыки для разработки ваших собственных ИИ приложений.

[На Хабре: https://habr.com/ru/articles/454034/](https://habr.com/ru/articles/454034/)
хhttps://www.youtube.com/playlist?list=PLfdVzZl6HHg9y9l6U5xUjqKS13rWoQPF4(https://www.youtube.com/playlist?list=PLfdVzZl6HHg9y9l6U5xUjqKS13rWoQPF4)


###### [в начало](#tvisskin)





