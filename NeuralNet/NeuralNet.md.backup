## [Проект нейронной сети на Python](NeuralNet/NeuralNet.ipynb)

### Весовые коэффициенты - сердце сети

Наиболее важная часть сети — весовые коэффициенты связей (веса). Они используются для расчета распространения сигналов в прямом направлении, а также обратного распространения ошибок, и именно весовые коэффициенты уточняются 
в попытке улучшить характеристики сети.

Для компактной записи весов будем использовать следующие матрицы:

```
• матрицу весов для связей между входным и скрытым слоями:
     W входной_скрытый,  размерностью hidden_nodes * input_nodes;
• другую матрицу для связей между скрытым и выходным слоями:
    W скрытый_выходной, размерностью output_nodes * hidden_nodes.
```

Начальные значения весовых коэффициентов должны быть небольшими и выбираться случайным образом. Следующая функция из пакета numpy генерирует массив случайных чисел в диапазоне от 0 до 1, где размерность массива равна rows * columns:

```
numpy.random.rand(rows,columns)
```
Так как собираемся использовать расширение пакета numpy, мы должны импортировать эту библиотеку в самом начале кода:

```
import numpy
```
Данный подход можно улучшить, поскольку весовые коэффициенты могут иметь не только положительные, но и отрицательные значения, и изменяться в пределах от -1,0 до +1,0 не достигая граничных пределов. Для простоты мы вычтем 0,5 из этих граничных значений, перейдя к диапазону значений от -0,5 до +0,5. Можно сказать - изящное решение:

```
m = numpy.random.rand(3,3) - 0.5

```
Создаем две матрицы начальных весов, используя значения переменных self.inodes, self.hnodes и self.onodes для задания соответствующих размеров каждой из матриц. Эти весовые коэффициенты составляют неотъемлемую часть нейронной сети и служат ее характеристиками на протяжении всей ее жизни.

```
self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5) 
self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5)
```
Это было простое, но популярное решение инициализации весовых коэффициентов.

Есть усовершенствованный подход к созданию случайных начальных значений весов. Для этого весовые коэффициенты выбираются из нормального распределения с центром в нуле и со стандартным отклонением, величина которого обратно пропорциональна корню квадратному из количества входящих связей на узел (такой контекст интуитивно кажется более подходящий для нейронной сети, но не факт).

Функция ***numpy.random.normal()*** поможет нам с извлечением выборки из нормального распределения. Ее параметрами являются центр распределения, стандартное отклонение и размер массива numpy, так как нам нужна матрица 
случайных чисел, а не одиночное число:

```
self.wih = numpy.random.normal(0.0, pow(self.hnodes, -0.5),(self.hnodes, self.inodes))
self.who = numpy.random.normal(0.0, pow(self.onodes, -0.5),(self.onodes, self.hnodes))
```
Как видно, центр нормального распределения установлен здесь в 0,0. Стандартное отклонение вычисляется по количеству узлов в следующем слое с помощью функции pow(self.hnodes, - 0.5), которая просто возводит количество узлов в степень -0 ,5. Последний параметр определяет конфигурацию массива numpy.

### Опрос сети

Опросом сети занимается функция query(), которая принимает в качестве аргумента входные данные нейронной сети и возвращает ее выходные данные. По аналогии с искусственным нейроном здесь передаются сигналы от узлов входного слоя через скрытый слой к узлам выходного слоя для получения выходных сигналов. 

В нашем случае, например, можно получить входящие сигналы для узлов скрытого слоя путем умножения матрицы весовых коэффициентов связей между входным и скрытым слоями на матрицу входных сигналов:

```
X скрытый = W скрытый_входной * I скрытый
```
При этом,  по мере распространения, сигналы сглаживаются через весовые коэффициенты связей между соответствующими узлами, а так же просчитывается сигмоида для уменьшения уровня выходных сигналов узлов.

В Python для передачи сигнала используется ***функция скалярного произведения*** матриц из библиотеки numpy к матрицам весов и входных сигналов:

```
hidden_inputs = numpy.dot(self.wih, inputs)
```

Библиотека ***scipy*** в Python содержит набор специальных функций, в том числе сигмоиду, которая называется expit (). Библиотека scipy импортируется точно так же, как и библиотека numpy:

```
import scipy.special
```
Определяем сигмоиду, как функцию активации нашей нейронной сети, следующим образом:

```
self.activation_function = lambda x: scipy.special.expit(x)
```
Здесь мы создали функцию, но вместо формы ***def имя()*** использовали волшебное слово ***lambda***,  как более короткий способ записи, называемый лямбда-выражением. Функция принимает аргумент х и возвращает scipy.special.expit(), а это есть не что иное, как сигмоида. Нашей функции мы присвоили имя self.activation _function(), так и буде


###### [в начало](#tvisskin)





